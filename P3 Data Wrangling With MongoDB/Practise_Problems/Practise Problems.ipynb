{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Csv Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = None\n",
    "    data = []\n",
    "    with open(datafile,'r') as f:\n",
    "        firstLine = f.readline()\n",
    "        name = firstLine.strip().split(',')[1]\n",
    "        secondLine = f.readline()\n",
    "        header = csv.reader(f)\n",
    "        data = (list(header))\n",
    "    # Do not change the line below\n",
    "    return (name.replace('\"',\"\"), data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Xlrd Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avgcoast': 10976.933460679751,\n",
       " 'maxtime': (2013, 8, 13, 17, 0, 0),\n",
       " 'maxvalue': 18779.025510000003,\n",
       " 'mintime': (2013, 2, 3, 4, 0, 0),\n",
       " 'minvalue': 6602.113898999982}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\"\"\"\n",
    "\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    #sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "\n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    \n",
    "    col_values = sheet.col_values(1,start_rowx=1,end_rowx=None)\n",
    "    \n",
    "    max_val = max(col_values)\n",
    "    min_val = min(col_values)\n",
    "    \n",
    "    max_pos = col_values.index(max_val) + 1\n",
    "    min_pos = col_values.index(min_val) + 1\n",
    "  \n",
    "    min_time = xlrd.xldate_as_tuple(sheet.cell_value(min_pos,0), 0)\n",
    "    max_time = xlrd.xldate_as_tuple(sheet.cell_value(max_pos,0), 0)\n",
    "    \n",
    "    data = {\n",
    "            'maxtime': max_time,\n",
    "            'maxvalue': max_val,\n",
    "            'mintime': min_time,\n",
    "            'minvalue': min_val,\n",
    "            'avgcoast': sum(col_values) / float(len(col_values))\n",
    "    }\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()\n",
    "\n",
    "parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'max_load': 18779.025510000003,\n",
       "  'station_name': 'COAST',\n",
       "  'time': (2013, 8, 13, 17, 0, 0)},\n",
       " {'max_load': 2380.1654089999956,\n",
       "  'station_name': 'EAST',\n",
       "  'time': (2013, 8, 5, 17, 0, 0)},\n",
       " {'max_load': 2281.2722140000024,\n",
       "  'station_name': 'FAR_WEST',\n",
       "  'time': (2013, 6, 26, 17, 0, 0)},\n",
       " {'max_load': 1544.7707140000005,\n",
       "  'station_name': 'NORTH',\n",
       "  'time': (2013, 8, 7, 17, 0, 0)},\n",
       " {'max_load': 24415.570226999993,\n",
       "  'station_name': 'NORTH_C',\n",
       "  'time': (2013, 8, 7, 18, 0, 0)},\n",
       " {'max_load': 5494.157645,\n",
       "  'station_name': 'SOUTHERN',\n",
       "  'time': (2013, 8, 8, 16, 0, 0)},\n",
       " {'max_load': 11433.30491600001,\n",
       "  'station_name': 'SOUTH_C',\n",
       "  'time': (2013, 8, 8, 18, 0, 0)},\n",
       " {'max_load': 1862.6137649999998,\n",
       "  'station_name': 'WEST',\n",
       "  'time': (2013, 8, 7, 17, 0, 0)}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "'''\n",
    "Find the time and value of max load for each of the regions\n",
    "COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "\n",
    "An example output can be seen in the \"example.csv\" file.\n",
    "'''\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    data = []\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    \n",
    "    for i in range(1,sheet.ncols-1):\n",
    "        temp = {}\n",
    "        temp[\"station_name\"] = (sheet.cell_value(0,i))\n",
    "        \n",
    "        col_values = sheet.col_values(i,start_rowx=1,end_rowx=None)\n",
    "        max_load = max(col_values)        \n",
    "        max_pos = col_values.index(max_load) + 1\n",
    "        max_date = xlrd.xldate_as_tuple(sheet.cell_value(max_pos,0),0)\n",
    "        \n",
    "        temp[\"max_load\"] = max_load\n",
    "        temp[\"time\"] = max_date\n",
    "        data.append(temp)\n",
    "            \n",
    "    return data\n",
    "parse_file(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COAST', 2013, 8, 13, 17, 18779.025510000003]\n",
      "['EAST', 2013, 8, 5, 17, 2380.1654089999956]\n",
      "['FAR_WEST', 2013, 6, 26, 17, 2281.2722140000024]\n",
      "['NORTH', 2013, 8, 7, 17, 1544.7707140000005]\n",
      "['NORTH_C', 2013, 8, 7, 18, 24415.570226999993]\n",
      "['SOUTHERN', 2013, 8, 8, 16, 5494.157645]\n",
      "['SOUTH_C', 2013, 8, 8, 18, 11433.30491600001]\n",
      "['WEST', 2013, 8, 7, 17, 1862.6137649999998]\n"
     ]
    }
   ],
   "source": [
    "data = parse_file(datafile)\n",
    "def save_file(data, filename):\n",
    "        with open(filename,'w') as f:\n",
    "            writer = csv.writer(f, delimiter='|')\n",
    "            writer.writerow(['Station','Year','Month','Day','Hour','Max Load'])\n",
    "            \n",
    "            for row in data:\n",
    "                temp = []\n",
    "                temp.append(row['station_name'])\n",
    "                temp.extend(list(row['time'])[:-2])\n",
    "                temp.append(row['max_load'])\n",
    "                print (temp)\n",
    "                writer.writerow(temp)\n",
    "                        \n",
    "save_file(data, \"example.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Using Element Tree Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'email': 'omer@extremegate.com',\n",
       "  'fnm': 'Omer',\n",
       "  'insr': ['I1'],\n",
       "  'snm': 'Mei-Dan'},\n",
       " {'email': 'mcarmont@hotmail.com',\n",
       "  'fnm': 'Mike',\n",
       "  'insr': ['I2'],\n",
       "  'snm': 'Carmont'},\n",
       " {'email': 'laver17@gmail.com',\n",
       "  'fnm': 'Lior',\n",
       "  'insr': ['I3', 'I4'],\n",
       "  'snm': 'Laver'},\n",
       " {'email': 'nyska@internet-zahav.net',\n",
       "  'fnm': 'Meir',\n",
       "  'insr': ['I3'],\n",
       "  'snm': 'Nyska'},\n",
       " {'email': 'kammarh@gmail.com',\n",
       "  'fnm': 'Hagay',\n",
       "  'insr': ['I8'],\n",
       "  'snm': 'Kammar'},\n",
       " {'email': 'gideon.mann.md@gmail.com',\n",
       "  'fnm': 'Gideon',\n",
       "  'insr': ['I3', 'I5'],\n",
       "  'snm': 'Mann'},\n",
       " {'email': 'barns.nz@gmail.com',\n",
       "  'fnm': 'Barnaby',\n",
       "  'insr': ['I6'],\n",
       "  'snm': 'Clarck'},\n",
       " {'email': 'eukots@gmail.com', 'fnm': 'Eugene', 'insr': ['I7'], 'snm': 'Kots'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        for child in author.findall('./insr'):\n",
    "            data[\"insr\"].append(child.attrib['iid'])\n",
    "       \n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "root = get_root(article_file)\n",
    "get_authors(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Beautiful Soup Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carriers:\n",
      "All AllUS AllForeign AS G4 AA 5Y DL MQ EV F9 HA B6 OO WN NK UA VX  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "\n",
    "def options(soup,id):\n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id=id)\n",
    "    for option in carrier_list.find_all('option'):\n",
    "        option_values.append(option['value'])\n",
    "    return option_values\n",
    "\n",
    "def print_list(label,codes):\n",
    "    print(\"\\n%s:\" %label)\n",
    "    newStr = ''\n",
    "    for c in codes:\n",
    "        newStr += (c) + \" \"\n",
    "    print (newStr,\"\\n\")\n",
    "    \n",
    "soup = BeautifulSoup(open(\"Data_Elements.html\"), \"lxml\")\n",
    "    \n",
    "codes = options(soup,'CarrierList')\n",
    "print_list(\"Carriers\",codes)\n",
    "    \n",
    "codes = options(soup,'AirportList')\n",
    "#print_list(\"Airports\",codes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Please note that the function 'make_request' is provided for your reference only.\n",
    "# You will not be able to to actually use it from within the Udacity web UI.\n",
    "# Your task is to process the HTML using BeautifulSoup, extract the hidden\n",
    "# form field values for \"__EVENTVALIDATION\" and \"__VIEWSTATE\" and set the appropriate\n",
    "# values in the data dictionary.\n",
    "# All your changes should be in the 'extract_data' function\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import json\n",
    "\n",
    "html_page = \"Data_Elements.html\"\n",
    "\n",
    "\n",
    "def extract_data(page):\n",
    "    data = {\"eventvalidation\": \"\",\n",
    "            \"viewstate\": \"\"}\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        data[\"eventvalidation\"] = soup.find(id=\"__EVENTVALIDATION\")['value']\n",
    "        data[\"viewstate\"] = soup.find(id=\"__VIEWSTATE\")['value']\n",
    "        print (data[\"eventvalidation\"])\n",
    "        print (data[\"viewstate\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "\n",
    "    r = requests.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "                    data={'AirportList': \"BOS\",\n",
    "                          'CarrierList': \"VX\",\n",
    "                          'Submit': 'Submit',\n",
    "                          \"__EVENTTARGET\": \"\",\n",
    "                          \"__EVENTARGUMENT\": \"\",\n",
    "                          \"__EVENTVALIDATION\": eventvalidation,\n",
    "                          \"__VIEWSTATE\": viewstate\n",
    "                    })\n",
    "\n",
    "    return r.text\n",
    "\n",
    "#extract_data(html_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "428933"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "s = requests.Session()\n",
    "r = s.get(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text,\"lxml\")\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewsate = viewstate_element['value']\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidation_element['value']\n",
    "\n",
    "r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",data={'AirportList':\"BOS\",\n",
    "                                                                                 'CarrierList':\"VX\",\n",
    "                                                                                 '__EVENTTARGET':\"\",\n",
    "                                                                                '__EVENTARGUMENT':\"\",\n",
    "                                                                                 '__EVENTVALIDATION':eventvalidation,\n",
    "                                                                                 '__VIEWSTATE':viewsate})\n",
    "\n",
    "f = open('virgin_and_logan_airport.html',\"w\")\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carrier List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['FL',\n",
       " 'AS',\n",
       " 'AA',\n",
       " 'MQ',\n",
       " '5Y',\n",
       " 'DL',\n",
       " 'EV',\n",
       " 'F9',\n",
       " 'HA',\n",
       " 'B6',\n",
       " 'OO',\n",
       " 'WN',\n",
       " 'NK',\n",
       " 'US',\n",
       " 'UA',\n",
       " 'VX']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Your task in this exercise is to modify 'extract_carrier()` to get a list of\n",
    "all airlines. Exclude all of the combination values like \"All U.S. Carriers\"\n",
    "from the data that you return. You should return a list of codes for the\n",
    "carriers.\n",
    "\n",
    "All your changes should be in the 'extract_carrier()' function. The\n",
    "'options.html' file in the tab above is a stripped down version of what is\n",
    "actually on the website, but should provide an example of what you should get\n",
    "from the full file.\n",
    "\n",
    "Please note that the function 'make_request()' is provided for your reference\n",
    "only. You will not be able to to actually use it from within the Udacity web UI.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_carriers(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id=\"CarrierList\")\n",
    "        for option in carrier_list.find_all('option'):\n",
    "            if option['value'] not in ['All','AllUS','AllForeign']:\n",
    "                data.append(option['value'])\n",
    "        print (len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "def make_request(data):\n",
    "    eventvalidation = data[\"eventvalidation\"]\n",
    "    viewstate = data[\"viewstate\"]\n",
    "    airport = data[\"airport\"]\n",
    "    carrier = data[\"carrier\"]\n",
    "\n",
    "    r = s.post(\"https://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "               data = ((\"__EVENTTARGET\", \"\"),\n",
    "                       (\"__EVENTARGUMENT\", \"\"),\n",
    "                       (\"__VIEWSTATE\", viewstate),\n",
    "                       (\"__VIEWSTATEGENERATOR\",viewstategenerator),\n",
    "                       (\"__EVENTVALIDATION\", eventvalidation),\n",
    "                       (\"CarrierList\", carrier),\n",
    "                       (\"AirportList\", airport),\n",
    "                       (\"Submit\", \"Submit\")))\n",
    "\n",
    "    return r.text\n",
    "\n",
    "\n",
    "\n",
    "extract_carriers(html_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Airport List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ATL',\n",
       " 'BWI',\n",
       " 'BOS',\n",
       " 'CLT',\n",
       " 'MDW',\n",
       " 'ORD',\n",
       " 'DFW',\n",
       " 'DEN',\n",
       " 'DTW',\n",
       " 'FLL',\n",
       " 'IAH',\n",
       " 'LAS',\n",
       " 'LAX',\n",
       " 'ABR',\n",
       " 'ABI']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Complete the 'extract_airports()' function so that it returns a list of airport\n",
    "codes, excluding any combinations like \"All\".\n",
    "\n",
    "Refer to the 'options.html' file in the tab above for a stripped down version\n",
    "of what is actually on the website. The test() assertions are based on the\n",
    "given file.\n",
    "\"\"\"\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html_page = \"options.html\"\n",
    "\n",
    "\n",
    "def extract_airports(page):\n",
    "    data = []\n",
    "\n",
    "    with open(page, \"r\") as html:\n",
    "        # do something here to find the necessary values\n",
    "        soup = BeautifulSoup(html, \"lxml\")\n",
    "        carrier_list = soup.find(id=\"AirportList\")\n",
    "        for option in carrier_list.find_all('option'):\n",
    "            if option['value'] not in ['All','AllMajors','AllOthers']:\n",
    "                data.append(option['value'])\n",
    "        print (len(data))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "extract_airports(html_page)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing All Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 815489, 'international': 92565},\n",
       "  'month': 10,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766775, 'international': 91342},\n",
       "  'month': 11,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 782175, 'international': 96881},\n",
       "  'month': 12,\n",
       "  'year': 2002},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 785651, 'international': 98053},\n",
       "  'month': 1,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 690750, 'international': 85965},\n",
       "  'month': 2,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 797634, 'international': 97929},\n",
       "  'month': 3,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766639, 'international': 89398},\n",
       "  'month': 4,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 789857, 'international': 87671},\n",
       "  'month': 5,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 798841, 'international': 95435},\n",
       "  'month': 6,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 832075, 'international': 102795},\n",
       "  'month': 7,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 831185, 'international': 102145},\n",
       "  'month': 8,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 782264, 'international': 90681},\n",
       "  'month': 9,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 818777, 'international': 91820},\n",
       "  'month': 10,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 766266, 'international': 91004},\n",
       "  'month': 11,\n",
       "  'year': 2003},\n",
       " {'airport': 'ATL',\n",
       "  'courier': 'FL',\n",
       "  'flights': {'domestic': 798879, 'international': 97094},\n",
       "  'month': 12,\n",
       "  'year': 2003}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_file(f):\n",
    "    \"\"\"\n",
    "    This function extracts data from the file given as the function argument in\n",
    "    a list of dictionaries. This is example of the data structure you should\n",
    "    return:\n",
    "\n",
    "    data = [{\"courier\": \"FL\",\n",
    "             \"airport\": \"ATL\",\n",
    "             \"year\": 2012,\n",
    "             \"month\": 12,\n",
    "             \"flights\": {\"domestic\": 100,\n",
    "                         \"international\": 100}\n",
    "            },\n",
    "            {\"courier\": \"...\"}\n",
    "    ]\n",
    "\n",
    "\n",
    "    Note - year, month, and the flight data should be integers.\n",
    "    You should skip the rows that contain the TOTAL data for a year.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    info = {}\n",
    "    info[\"courier\"], info[\"airport\"] = f[:6].split(\"-\")\n",
    "    # Note: create a new dictionary for each entry in the output data list.\n",
    "    # If you use the info dictionary defined here each element in the list \n",
    "    # will be a reference to the same info dictionary.\n",
    "    with open(f, \"r\") as html:\n",
    "        soup = BeautifulSoup(html,\"lxml\")\n",
    "        table_data = soup.find(id = \"DataGrid1\")\n",
    "        for tr in table_data.find_all('tr',class_ = \"dataTDRight\"):\n",
    "            \n",
    "            temp_lst = []\n",
    "            for td in tr.find_all('td'):\n",
    "                temp_lst.append(td.text)\n",
    "            if temp_lst[1] != \"TOTAL\":\n",
    "                temp_dict = {}\n",
    "                temp_dict[\"courier\"] = info[\"courier\"]\n",
    "                temp_dict[\"airport\"] = info[\"airport\"]\n",
    "                temp_dict[\"year\"] = int(temp_lst[0])\n",
    "                temp_dict[\"month\"] = int(temp_lst[1])\n",
    "                temp={}\n",
    "                temp[\"domestic\"] = int(temp_lst[2].replace(',',''))\n",
    "                temp[\"international\"] = int(temp_lst[3].replace(',',''))\n",
    "                temp_dict[\"flights\"] = temp\n",
    "                data.append(temp_dict)\n",
    "               \n",
    "    return data\n",
    "\n",
    "process_file(\"FL-ATL.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting A File With Multiple Xmls Docs Into Multiple Files with Single Xml Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# So, the problem is that the gigantic file is actually not a valid XML, because\n",
    "# it has several root elements, and XML declarations.\n",
    "# It is, a matter of fact, a collection of a lot of concatenated XML documents.\n",
    "# So, one solution would be to split the file into separate documents,\n",
    "# so that you can process the resulting files as valid XML documents.\n",
    "\n",
    "PATENTS = 'patent.data'\n",
    "\n",
    "def split_file(filename):\n",
    "    \"\"\"\n",
    "    Split the input file into separate files, each containing a single patent.\n",
    "    As a hint - each patent declaration starts with the same line that was\n",
    "    causing the error found in the previous exercises.\n",
    "    \n",
    "    The new files should be saved with filename in the following format:\n",
    "    \"{}-{}\".format(filename, n) where n is a counter, starting from 0.\n",
    "    \"\"\"\n",
    "    count  = 0 \n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith('<?xml') :\n",
    "                fname = \"{}-{}\".format(filename, count)\n",
    "                count  += 1\n",
    "                with open(fname,'w') as f1:\n",
    "                    f1.write(line)\n",
    "                \n",
    "split_file(PATENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Validity Problem Solve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
    "The following things should be done:\n",
    "- check if the field \"productionStartYear\" contains a year\n",
    "- check if the year is in range 1886-2014\n",
    "- convert the value of the field to be just a year (not full datetime)\n",
    "- the rest of the fields and values should stay the same\n",
    "- if the value of the field is a valid year in the range as described above,\n",
    "  write that line to the output_good file\n",
    "- if the value of the field is not a valid year as described above, \n",
    "  write that line to the output_bad file\n",
    "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
    "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
    "  They will take care of dealing with the header.\n",
    "\n",
    "You can write helper functions for checking the data and writing the files, but we will call only the \n",
    "'process_file' with 3 arguments (inputfile, output_good, output_bad).\n",
    "\"\"\"\n",
    "import csv\n",
    "import pprint\n",
    "\n",
    "INPUT_FILE = 'autos.csv'\n",
    "OUTPUT_GOOD = 'autos-valid.csv'\n",
    "OUTPUT_BAD = 'FIXME-autos.csv'\n",
    "\n",
    "def process_file(input_file, output_good, output_bad):\n",
    "\n",
    "    with open(input_file, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        header = reader.fieldnames.\n",
    "        \n",
    "        good_data = []\n",
    "        bad_data = []\n",
    "        \n",
    "        for row in reader:\n",
    "            if 'dbpedia' in row['URI']:\n",
    "                temp_year = row['productionStartYear']\n",
    "                if temp_year is not None and temp_year != '' and temp_year != 'NULL':\n",
    "                    row['productionStartYear'] = year = int(temp_year.split('-')[0])\n",
    "                    if year > 1886 and year < 2014:\n",
    "                        good_data.append(row)\n",
    "                    else:\n",
    "                        bad_data.append(row)\n",
    "                else:\n",
    "                    bad_data.append(row)\n",
    "    \n",
    "        writeToFile(output_good,good_data,header)\n",
    "        writeToFile(output_bad,bad_data,header)\n",
    "\n",
    "    \n",
    "def writeToFile(filename,YOURDATA,header):\n",
    "    with open(filename, \"w\") as g:\n",
    "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
    "        writer.writeheader()\n",
    "        for row in YOURDATA:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set : Data Quality\n",
    "\n",
    "###  1. Auditing Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'areaCode': {int, str, NoneType},\n",
       " 'areaLand': {list, NoneType, float},\n",
       " 'areaMetro': {NoneType, float},\n",
       " 'areaUrban': {NoneType, float},\n",
       " 'elevation': {list, NoneType, float},\n",
       " 'governmentType_label': {str, NoneType},\n",
       " 'homepage': {str, NoneType},\n",
       " 'isPartOf_label': {list, NoneType, str},\n",
       " 'maximumElevation': {NoneType},\n",
       " 'minimumElevation': {NoneType},\n",
       " 'name': {str, NoneType, list},\n",
       " 'populationDensity': {list, NoneType, float},\n",
       " 'populationTotal': {int, NoneType},\n",
       " 'timeZone_label': {str, NoneType},\n",
       " 'utcOffset': {int, str, NoneType, list},\n",
       " 'wgs84_pos#lat': {float},\n",
       " 'wgs84_pos#long': {float}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
    "the datatypes that can be found in some particular fields in the dataset.\n",
    "The possible types of values can be:\n",
    "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
    "- list, if the value starts with \"{\"\n",
    "- int, if the value can be cast to int\n",
    "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
    "   For example, '3.23e+07' should be considered a float because it can be cast\n",
    "   as float but int('3.23e+07') will throw a ValueError\n",
    "- 'str', for all other values\n",
    "\n",
    "The audit_file function should return a dictionary containing fieldnames and a \n",
    "SET of the types that can be found in the field. e.g.\n",
    "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
    " \"field2\": set([type(str())]),\n",
    "  ....\n",
    "}\n",
    "The type() function returns a type object describing the argument given to the \n",
    "function. You can also use examples of objects to create type objects, e.g.\n",
    "type(1.1) for a float: see the test function below for examples.\n",
    "\n",
    "Note that the first three rows (after the header row) in the cities.csv file\n",
    "are not actual data points. The contents of these rows should note be included\n",
    "when processing data types. Be sure to include functionality in your code to\n",
    "skip over or detect these rows.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
    "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
    "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
    "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
    "\n",
    "def isNoneType(val):\n",
    "    if val == '' or val == 'NULL':\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "    \n",
    "def isArray(val):\n",
    "    if val.startswith('{'):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def isInt(val):\n",
    "    try:\n",
    "        int(val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "def isFloat(val):\n",
    "    try:\n",
    "        float(val)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False        \n",
    "\n",
    "def audit_file(filename, fields):\n",
    "    fieldtypes = {}\n",
    "    # YOUR CODE HERE\n",
    "    for val in fields:\n",
    "        fieldtypes[val] = set()\n",
    "    \n",
    "    reader = csv.DictReader(open('cities.csv','r'))\n",
    "    \n",
    "    for i in range(3):\n",
    "        next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        #print(row['areaLand'])\n",
    "        for field in fields:\n",
    "            if isNoneType(row[field]):\n",
    "                fieldtypes[field].add(type(None))\n",
    "            elif isArray(row[field]):\n",
    "                fieldtypes[field].add( type([]))\n",
    "            elif isInt(row[field]):\n",
    "                fieldtypes[field].add(type(1))\n",
    "            elif isFloat(row[field]):\n",
    "                fieldtypes[field].add(type(1.1))    \n",
    "            else:\n",
    "                fieldtypes[field].add(type('aa')) \n",
    "\n",
    "    return fieldtypes\n",
    "\n",
    "audit_file(CITIES, FIELDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Fixing The Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing three example results:\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "101787000.0\n",
      "31597900.0\n",
      "55166700.0\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "In this problem set you work with cities infobox data, audit it, come up with a\n",
    "cleaning idea and then clean it up.\n",
    "\n",
    "Since in the previous quiz you made a decision on which value to keep for the\n",
    "\"areaLand\" field, you now know what has to be done.\n",
    "\n",
    "Finish the function fix_area(). It will receive a string as an input, and it\n",
    "has to return a float representing the value of the area or None.\n",
    "You have to change the function fix_area. You can use extra functions if you\n",
    "like, but changes to process_file will not be taken into account.\n",
    "The rest of the code is just an example on how this function can be used.\n",
    "\"\"\"\n",
    "import codecs\n",
    "import csv\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "CITIES = 'cities.csv'\n",
    "\n",
    "def countZeros(val):\n",
    "    count  = 0\n",
    "    while val != 0:\n",
    "        rem = val % 10\n",
    "        if rem != 0:\n",
    "            return count\n",
    "        else:\n",
    "            count  +=  1\n",
    "            val = val // 10\n",
    "\n",
    "def mostSignificant(areaList):\n",
    "    \n",
    "    if countZeros(areaList[0]) < countZeros(areaList[1]):\n",
    "        return areaList[0]\n",
    "    else:\n",
    "        return areaList[1]\n",
    "\n",
    "\n",
    "def fix_area(area):\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    res = None\n",
    "    if area is None or area == '' or area == 'NULL' :\n",
    "        return None\n",
    "    elif area.startswith('{'):\n",
    "        area = area.replace('{','').replace('}','').split('|')\n",
    "        area = [float(val) for val in area]\n",
    "        return mostSignificant(area)\n",
    "    \n",
    "    else :\n",
    "        return float(area)\n",
    "\n",
    "def process_file(filename):\n",
    "    # CHANGES TO THIS FUNCTION WILL BE IGNORED WHEN YOU SUBMIT THE EXERCISE\n",
    "    data = []\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        #skipping the extra metadata\n",
    "        for i in range(3):\n",
    "            next(reader)\n",
    "\n",
    "        # processing file\n",
    "        for line in reader:\n",
    "            # calling your function to fix the area value\n",
    "            if \"areaLand\" in line:\n",
    "                line[\"areaLand\"] = fix_area(line[\"areaLand\"])\n",
    "            data.append(line)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    data = process_file(CITIES)\n",
    "\n",
    "    print(\"Printing three example results:\")\n",
    "    for n in range(9):\n",
    "        pprint.pprint(data[n][\"areaLand\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
